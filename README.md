# Proyecto Final de Machine Learning - Database II
## An√°lisis del Dataset Yeast

Este proyecto implementa un an√°lisis completo de Machine Learning sobre el dataset Yeast (Protein Localization Sites), aplicando m√∫ltiples t√©cnicas vistas en el curso.

## üìÅ Estructura del Proyecto

```
ML_ProyectoFinalDB2/
‚îú‚îÄ‚îÄ DATABASE/
‚îÇ   ‚îú‚îÄ‚îÄ yeast.data          # Dataset principal
‚îÇ   ‚îî‚îÄ‚îÄ yeast.names         # Descripci√≥n del dataset
‚îú‚îÄ‚îÄ yeast_ml_analysis.ipynb # Notebook principal con todo el an√°lisis
‚îú‚îÄ‚îÄ requirements.txt        # Dependencias del proyecto
‚îî‚îÄ‚îÄ README.md              # Este archivo
```

## üìä Dataset

**Yeast Protein Localization Sites**
- **Instancias**: 1,484
- **Caracter√≠sticas**: 8 num√©ricas + 1 nombre
- **Clases**: 10 (sitios de localizaci√≥n de prote√≠nas)
- **Fuente**: UCI Machine Learning Repository

### Caracter√≠sticas del Dataset:
1. `mcg`: McGeoch's method for signal sequence recognition
2. `gvh`: von Heijne's method for signal sequence recognition  
3. `alm`: Score of ALOM membrane spanning region prediction
4. `mit`: Discriminant analysis of mitochondrial proteins
5. `erl`: Presence of "HDEL" substring
6. `pox`: Peroxisomal targeting signal
7. `vac`: Discriminant analysis of vacuolar proteins
8. `nuc`: Discriminant analysis of nuclear proteins

### Clases:
- CYT (cytosolic/cytoskeletal): 463
- NUC (nuclear): 429
- MIT (mitochondrial): 244
- ME3 (membrane protein, no signal): 163
- ME2 (membrane protein, uncleaved): 51
- ME1 (membrane protein, cleaved): 44
- EXC (extracellular): 37
- VAC (vacuolar): 30
- POX (peroxisomal): 20
- ERL (endoplasmic reticulum): 5

## üõ†Ô∏è M√©todos Implementados

### 1. Imputaci√≥n de Valores Faltantes
- **Forward/Backward Fill**: Imputaci√≥n secuencial
- **KNNImputer**: Imputaci√≥n basada en k-vecinos m√°s cercanos

### 2. Regularizaci√≥n
- **Ridge (L2 Regularization)**: Penalizaci√≥n cuadr√°tica
- **Lasso (L1 Regularization)**: Penalizaci√≥n absoluta con selecci√≥n de features
- **Elastic Net**: Combinaci√≥n de L1 y L2

### 3. Detecci√≥n de Outliers (PyOD)
- **KNN**: K-Nearest Neighbors para detecci√≥n de anomal√≠as
- **Isolation Forest**: Aislamiento aleatorio de observaciones
- **LOF**: Local Outlier Factor

### 4. Selecci√≥n de Caracter√≠sticas
- **Boruta**: M√©todo basado en Random Forest con shadow features
- **Lasso**: Selecci√≥n autom√°tica mediante coeficientes L1
- **Stepwise (Forward Selection)**: B√∫squeda secuencial hacia adelante

### 5. √Årboles de Decisi√≥n
- **√Årbol de Clasificaci√≥n**: Con optimizaci√≥n de hiperpar√°metros v√≠a GridSearch
- **√Årbol de Regresi√≥n**: Para predicci√≥n continua de clases codificadas

### 6. Optimizaci√≥n de Hiperpar√°metros
- **GridSearchCV**: B√∫squeda exhaustiva de hiperpar√°metros √≥ptimos
- **Cross-Validation**: Validaci√≥n cruzada de 5 folds

## üì¶ Instalaci√≥n

### Requisitos Previos
- Python 3.8 o superior
- pip

### Pasos de Instalaci√≥n

1. Clonar o descargar el repositorio

2. Instalar las dependencias:
```bash
pip install -r requirements.txt
```

### Dependencias Principales
```
pandas==2.0.3
numpy==1.24.3
scikit-learn==1.3.0
matplotlib==3.7.2
seaborn==0.12.2
pyod==1.1.0
boruta==0.3
mlxtend==0.22.0
jupyter==1.0.0
imbalanced-learn==0.11.0
```

## üöÄ Uso

### Ejecutar el An√°lisis Completo

1. Abrir Jupyter Notebook:
```bash
jupyter notebook yeast_ml_analysis.ipynb
```

2. Ejecutar todas las celdas secuencialmente (Cell ‚Üí Run All)

### Estructura del Notebook

El notebook est√° organizado en las siguientes secciones:

1. **Importar Librer√≠as**: Carga de todas las dependencias necesarias
2. **Carga y Exploraci√≥n de Datos**: An√°lisis exploratorio inicial
3. **Preparaci√≥n de Datos**: Separaci√≥n de features y target
4. **Imputaci√≥n de Valores**: Comparaci√≥n de m√©todos de imputaci√≥n
5. **Detecci√≥n de Outliers**: Aplicaci√≥n de PyOD
6. **Selecci√≥n de Caracter√≠sticas**: Boruta, Lasso y Stepwise
7. **Modelos de Regularizaci√≥n**: Ridge, Lasso y Elastic Net
8. **√Årboles de Decisi√≥n**: Clasificaci√≥n y regresi√≥n
9. **Comparativa Final**: Evaluaci√≥n de todos los modelos
10. **Conclusiones**: Resumen de resultados

## üìà Resultados Esperados

El notebook genera:

- **Visualizaciones**:
  - Distribuci√≥n de clases
  - Matriz de correlaci√≥n
  - Comparaci√≥n de m√©todos de imputaci√≥n
  - Detecci√≥n de outliers (m√∫ltiples m√©todos)
  - Diagrama de Venn para selecci√≥n de caracter√≠sticas
  - √Årbol de decisi√≥n visualizado
  - Matriz de confusi√≥n
  - Gr√°ficas comparativas de modelos

- **M√©tricas**:
  - **Regresi√≥n**: MSE, MAE, R¬≤
  - **Clasificaci√≥n**: Accuracy, Precision, Recall, F1-Score
  - Importancia de caracter√≠sticas

- **Comparativa Final**:
  - Tabla resumen con todos los modelos
  - Recomendaciones sobre qu√© modelo usar

## üìù An√°lisis Realizados

### Preprocesamiento
- Introducci√≥n artificial de valores faltantes (5%)
- Comparaci√≥n de t√©cnicas de imputaci√≥n
- Detecci√≥n y remoci√≥n de outliers por consenso

### Feature Engineering
- Selecci√≥n mediante 3 m√©todos diferentes
- Identificaci√≥n de caracter√≠sticas m√°s relevantes
- An√°lisis de importancia

### Modelado
- Optimizaci√≥n de hiperpar√°metros con GridSearch
- Validaci√≥n cruzada de 5 folds
- Comparaci√≥n exhaustiva de modelos

## üéØ Conclusiones Principales

1. **Imputaci√≥n**: KNNImputer demostr√≥ ser m√°s robusto que Forward/Backward Fill
2. **Outliers**: El consenso de m√∫ltiples m√©todos mejora la calidad del dataset
3. **Selecci√≥n de Features**: Los 3 m√©todos identificaron caracter√≠sticas comunes relevantes
4. **Regularizaci√≥n**: Elastic Net ofrece un buen balance entre Ridge y Lasso
5. **√Årboles**: La optimizaci√≥n de hiperpar√°metros mejora significativamente el desempe√±o

## üë• Autores

Proyecto Final - Base de Datos II  
Universidad de Lima - Ciclo VIII  
Machine Learning

## üìÑ Licencia

Este proyecto es parte de un trabajo acad√©mico.

## üîó Referencias

- Dataset: [UCI Machine Learning Repository - Yeast](https://archive.ics.uci.edu/ml/datasets/Yeast)
- Nakai, K., & Kanehisa, M. (1991). Expert system for predicting protein localization sites in gram-negative bacteria
- Horton, P., & Nakai, K. (1996). A probabilistic classification system for predicting the cellular localization sites of proteins

## üìß Contacto

Para preguntas o sugerencias sobre este proyecto, contactar al equipo de desarrollo.

---

**Nota**: Este es un proyecto acad√©mico que demuestra la aplicaci√≥n pr√°ctica de t√©cnicas de Machine Learning vistas en el curso.
